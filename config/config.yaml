# Model params
model: # Model params
  mu: 7  #
  threshold: 0.95
  choose_threshold_on_val: False
  choose_threshold_on_val_freq: 128  # n data-steps, after which threshold will be updated
  choose_threshold_on_val_acc: 0.95  # Choosing threshold, which guarantees this accuracy
  lambda_u: 1.0
  ema_decay: 0.9
  T: 50
  certainty_strategy: SoftMax #  SoftMax / MeanSoftmax / PECertainty / BALDCertainty / Margin / Entropy
  multi_strategy: False  # If True - all strategies' statistics are logged, decision is made with certainty_strategy
  drop_type: Dropout  # Dropout / DropConnect / AlphaDropout / AfterBNDropout / UniformDropout
  drop_rate: 0.0

# Data params
data:
  source: ???   #  CIFAR10 / CIFAR100 / SVHN / STL10
  n_labelled: ???    # Number of labelled images to use from train, all the other images become unlabelled
  val_ratio: 0.1  # The size of the validation set based on the train-set
  steps_per_epoch: 1024  # Data steps per epoch: min (between lab and unlab dataloaders) / max (between lab and unlab dataloaders) / x (int)
  batch_size:  # Batch size per GPU/CPU for training
    train: 64
    val: 64
    test: 64
  weak_aug:
    flip: True
    random_pad_and_crop: True

# Experiment
exp:
  check_val_every_n_epoch: 1   # Evaluate validation subset n-th epoch
  logging: True   # Logging to MlFlow
  mlflow_uri: http://127.0.0.1:5004
  log_artifacts: False  # Logging the artifacts, e.g. images
  log_ul_w_statistics: image  # Logging the statistics about unlabelled weakly augmented data: batch / image / False
  log_ul_s_statistics: image  # Logging the statistics about unlabelled strongly data: batch / image / False
  log_val_statistics: image  # Logging the statistics about val data: batch / image / False
  log_statistics_freq: 10  # CSV-files saving frequency
  max_epochs: 1024  # Total number of training epochs to perform.
  early_stopping: False
  early_stopping_patience: 100  #  Number of epochs to wait for early stopping, used if early_stopping is True
  seed: 42  # random seed for initialization
  checkpoint: True  # Saving best model in RAM and then using it for test
  gpus: '-1'
  drop_last_batch: True  # Ignoring the last not full batch while training
  precision: 16  # 16 / 32

# Optimizer
optimizer:
  momentum: 0.9
  nesterov: True
  weight_decay: ???  # Dataset-specific parameter - look to the paper for the specific value
  lr: 0.03  # The initial learning rate for Adam
  auto_lr_find: False  # Auto lr-finding before training
  warmup_steps: 0  # Warm-up steps for Cosine LR scheduler

# Hydra defaults
defaults:
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog